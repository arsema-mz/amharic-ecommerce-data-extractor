{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4ded81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved labeled data to data/labeled_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_conll(filepath):\n",
    "    sentences = []\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    token, label = parts[0], parts[-1]\n",
    "                    sentence.append((token, label))\n",
    "        if sentence:  # Catch any trailing sentence\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "# âœ… 1. Load the data\n",
    "conll_data = parse_conll(\"../Labeled_data.con11\") \n",
    "\n",
    "# âœ… 2. Flatten it into token-label rows\n",
    "flat_data = []\n",
    "for sentence in conll_data:\n",
    "    for token, label in sentence:\n",
    "        flat_data.append({\"token\": token, \"label\": label})\n",
    "\n",
    "# âœ… 3. Create a DataFrame\n",
    "df = pd.DataFrame(flat_data)\n",
    "\n",
    "# âœ… 4. Save as CSV\n",
    "df.to_csv(\"../data/labeled_data.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"âœ… Saved labeled data to data/labeled_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff06b653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['token', 'label'],\n",
      "    num_rows: 30\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'token': ['40,41,42,43', 'Price', '3700', 'birr', 'ðŸ“ŒáŠ á‹µáˆ«áˆ»-áˆœáŠ­áˆ²áŠ®'],\n",
       " 'label': ['B-PRICE', 'I-PRICE', 'I-PRICE', 'I-PRICE', 'B-LOC']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV into a pandas DataFrame\n",
    "df = pd.read_csv(\"../data/labeled_data.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Group tokens and labels by sentence if needed\n",
    "# For now, we'll assume the data is flat â€” i.e., a long list of token/label rows\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Peek at it\n",
    "print(dataset)\n",
    "dataset[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8801e9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('40,41,42,43 Price 3700 birr ðŸ“ŒáŠ á‹µáˆ«áˆ»-áˆœáŠ­áˆ²áŠ® áŒ€áˆ­á‰£ áˆ˜á‹šá‹µ á•áˆ‹á‹› EthioBrand Reebok Winterized 40,41,42,43,44 Price 3600 birr ðŸ“ŒáŠ á‹µáˆ«áˆ»-áˆœáŠ­áˆ²áŠ® áŒ€áˆ­á‰£ áˆ˜á‹šá‹µ á•áˆ‹á‹› EthioBrand â„¹ï¸â„¹ï¸â„¹ï¸ Ice cube Bottle ice cube maker pitcher ice cubes', {'entities': [(0, 11, 'PRICE'), (12, 17, 'PRICE'), (18, 22, 'PRICE'), (23, 27, 'PRICE'), (28, 38, 'LOC'), (39, 42, 'Product'), (43, 46, 'Product'), (47, 50, 'Product'), (51, 61, 'Product'), (62, 68, 'Product'), (69, 79, 'Product'), (80, 94, 'PRICE'), (95, 100, 'PRICE'), (101, 105, 'PRICE'), (106, 110, 'PRICE'), (111, 121, 'LOC'), (122, 125, 'Product'), (126, 129, 'Product'), (130, 133, 'Product'), (134, 144, 'Product'), (145, 151, 'Product'), (152, 155, 'Product'), (156, 160, 'Product'), (161, 167, 'Product'), (168, 171, 'Product'), (172, 176, 'Product'), (177, 182, 'Product'), (183, 190, 'Product'), (191, 194, 'Product'), (195, 200, 'Product')]})\n",
      "âœ… Converted 1 sentences into spaCy format.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"../data/labeled_data.csv\")  # Adjust path\n",
    "\n",
    "sentences = []\n",
    "sentence = []\n",
    "for _, row in df.iterrows():\n",
    "    token = str(row[\"token\"]).strip()\n",
    "    label = row[\"label\"].strip()\n",
    "\n",
    "    if token == \"\" or pd.isna(token):\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "    else:\n",
    "        sentence.append((token, label))\n",
    "\n",
    "# Append last sentence if not empty\n",
    "if sentence:\n",
    "    sentences.append(sentence)\n",
    "\n",
    "# Convert to spaCy format\n",
    "train_data = []\n",
    "\n",
    "for sent in sentences:\n",
    "    text = \"\"\n",
    "    entities = []\n",
    "    offset = 0\n",
    "\n",
    "    for token, label in sent:\n",
    "        token = str(token)\n",
    "        start = len(text)\n",
    "        text += token + \" \"\n",
    "        end = len(text) - 1  # subtract last space\n",
    "\n",
    "        if label != \"O\":\n",
    "            entity_type = label.split(\"-\")[1]\n",
    "            entities.append((start, end, entity_type))\n",
    "\n",
    "    train_data.append((text.strip(), {\"entities\": entities}))\n",
    "\n",
    "# Print a few examples\n",
    "for ex in train_data[:3]:\n",
    "    print(ex)\n",
    "\n",
    "# Optional: save to .py for training\n",
    "import json\n",
    "with open(\"spacy_train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Converted {len(train_data)} sentences into spaCy format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91623de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Losses {'ner': np.float32(33.428574)}\n",
      "Epoch 2: Losses {'ner': np.float32(32.80156)}\n",
      "Epoch 3: Losses {'ner': np.float32(32.115204)}\n",
      "Epoch 4: Losses {'ner': np.float32(31.490658)}\n",
      "Epoch 5: Losses {'ner': np.float32(30.671091)}\n",
      "Epoch 6: Losses {'ner': np.float32(29.570364)}\n",
      "Epoch 7: Losses {'ner': np.float32(28.702402)}\n",
      "Epoch 8: Losses {'ner': np.float32(27.67277)}\n",
      "Epoch 9: Losses {'ner': np.float32(26.696175)}\n",
      "Epoch 10: Losses {'ner': np.float32(25.662355)}\n",
      "âœ… Model trained and saved to amharic_ner_model/\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Load blank Amharic-compatible model (multilingual or English as fallback)\n",
    "nlp = spacy.blank(\"xx\")  # xx = multilingual\n",
    "\n",
    "# Create NER pipe\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "labels = set()\n",
    "\n",
    "# Load training data\n",
    "with open(\"spacy_train_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    TRAIN_DATA = json.load(f)\n",
    "\n",
    "# Add labels\n",
    "for text, ann in TRAIN_DATA:\n",
    "    for ent in ann[\"entities\"]:\n",
    "        labels.add(ent[2])\n",
    "\n",
    "for label in labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "# Training\n",
    "optimizer = nlp.initialize()\n",
    "for i in range(10):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    for text, ann in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, ann)\n",
    "        nlp.update([example], losses=losses, drop=0.35)\n",
    "    print(f\"Epoch {i+1}: Losses {losses}\")\n",
    "\n",
    "# Save model\n",
    "nlp.to_disk(\"amharic_ner_model\")\n",
    "print(\"âœ… Model trained and saved to amharic_ner_model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae756dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40,41,42,43 -> Product\n",
      "Price -> Product\n",
      "3700 -> Product\n",
      "birr -> Product\n",
      "ðŸ“Œ -> Product\n",
      "áŠ á‹µáˆ«áˆ» -> Product\n",
      "- -> Product\n",
      "áˆœáŠ­áˆ²áŠ® -> Product\n",
      "áŒ€áˆ­á‰£ -> Product\n",
      "á•áˆ‹á‹› -> Product\n",
      "Ice -> Product\n",
      "cube -> Product\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load your trained model\n",
    "nlp = spacy.load(\"amharic_ner_model\")\n",
    "\n",
    "# Example Amharic message\n",
    "text = \"40,41,42,43 Price 3700 birr ðŸ“ŒáŠ á‹µáˆ«áˆ»-áˆœáŠ­áˆ²áŠ® áŒ€áˆ­á‰£ á•áˆ‹á‹› Ice cube\"\n",
    "\n",
    "# Run NER prediction\n",
    "doc = nlp(text)\n",
    "\n",
    "# Show entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4792ae83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Converted 1 sentences into spaCy format.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"../data/labeled_data.csv\")\n",
    "\n",
    "# Initialize\n",
    "data = []\n",
    "sentence = []\n",
    "labels = []\n",
    "current_index = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    token = str(row['token']).strip()\n",
    "    label = str(row['label']).strip()\n",
    "\n",
    "    if pd.isna(token) or pd.isna(label):\n",
    "        continue\n",
    "\n",
    "    if label == 'O':\n",
    "        sentence.append(token)\n",
    "        current_index += len(token) + 1  # +1 for space\n",
    "        continue\n",
    "\n",
    "    # Detect sentence breaks (you can improve this logic)\n",
    "    if token in [\".\", \"!\", \"?\"] and sentence:\n",
    "        joined = \" \".join(sentence)\n",
    "        data.append((joined, {\"entities\": labels}))\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        current_index = 0\n",
    "        continue\n",
    "\n",
    "    sentence.append(token)\n",
    "    start = current_index\n",
    "    end = start + len(token)\n",
    "    labels.append((start, end, label.split(\"-\")[-1]))  # Use PRICE, LOC, Product\n",
    "    current_index = end + 1  # +1 for space\n",
    "\n",
    "# Final one\n",
    "if sentence:\n",
    "    joined = \" \".join(sentence)\n",
    "    data.append((joined, {\"entities\": labels}))\n",
    "\n",
    "print(f\"âœ… Converted {len(data)} sentences into spaCy format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aab14d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Losses {'ner': np.float32(33.428574)}\n",
      "Epoch 2: Losses {'ner': np.float32(32.810528)}\n",
      "Epoch 3: Losses {'ner': np.float32(31.664017)}\n",
      "Epoch 4: Losses {'ner': np.float32(29.919365)}\n",
      "Epoch 5: Losses {'ner': np.float32(27.517132)}\n",
      "Epoch 6: Losses {'ner': np.float32(24.655592)}\n",
      "Epoch 7: Losses {'ner': np.float32(21.719816)}\n",
      "Epoch 8: Losses {'ner': np.float32(19.156385)}\n",
      "Epoch 9: Losses {'ner': np.float32(16.41469)}\n",
      "Epoch 10: Losses {'ner': np.float32(50.462124)}\n",
      "âœ… Model trained and saved to amharic_ner_model/\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch\n",
    "import random\n",
    "\n",
    "# Load blank model\n",
    "nlp = spacy.blank(\"xx\")  # multilingual\n",
    "\n",
    "# Add NER pipeline\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "for _, ann in data:\n",
    "    for ent in ann[\"entities\"]:\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Train the model\n",
    "nlp.begin_training()\n",
    "for epoch in range(10):\n",
    "    random.shuffle(data)\n",
    "    losses = {}\n",
    "    batches = minibatch(data, size=2)\n",
    "    for batch in batches:\n",
    "        examples = []\n",
    "        for text, ann in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            examples.append(Example.from_dict(doc, ann))\n",
    "        nlp.update(examples, losses=losses)\n",
    "    print(f\"Epoch {epoch+1}: Losses\", losses)\n",
    "\n",
    "# Save model\n",
    "nlp.to_disk(\"amharic_ner_model\")\n",
    "print(\"âœ… Model trained and saved to amharic_ner_model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4021b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ice Product\n",
      "cube Product\n",
      "maker Product\n",
      "pitcher Product\n",
      "3700 Product\n",
      "birr Product\n",
      "ðŸ“ŒáŠ á‹µáˆ«áˆ» LOC\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"amharic_ner_model\")\n",
    "\n",
    "doc = nlp(\"Ice cube maker pitcher 3700 birr ðŸ“ŒáŠ á‹µáˆ«áˆ»-áˆœáŠ­áˆ²áŠ®\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393e94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaForTokenClassification, Trainer, TrainingArguments\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = XLMRobertaForTokenClassification.from_pretrained(\"xlm-roberta-base\")\n",
    "# Fine-tune with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d12d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "# Fine-tune similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# Fine-tune similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ae0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Generate predictions and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3100938",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(\u001b[43mmodel\u001b[49m)\n\u001b[0;32m      3\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer(data)\n\u001b[0;32m      4\u001b[0m shap\u001b[38;5;241m.\u001b[39msummary_plot(shap_values, features\u001b[38;5;241m=\u001b[39mdata)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(data)\n",
    "shap.summary_plot(shap_values, features=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "exp = explainer.explain_instance(text_instance, model.predict)\n",
    "exp.show_in_notebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
